base_model: /path/to/base-model
model_type: AutoModelForCausalLM
tokenizer_type: AutoTokenizer

load_in_4bit: true
adapter: qlora

datasets:
  - path: /absolute/path/to/data/splits/train.jsonl
    type: chatml

output_dir: ./artifacts/qlora-chat
sequence_len: 4096
sample_packing: true

lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_linear: true

gradient_accumulation_steps: 4
micro_batch_size: 2
num_epochs: 2
learning_rate: 2e-4
warmup_steps: 50

bf16: true
tf32: true
flash_attention: true

logging_steps: 10
save_steps: 200
eval_steps: 200
